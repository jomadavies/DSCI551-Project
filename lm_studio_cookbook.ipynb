{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo and instructions for using LM Studio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LM studio is an application used to locally host LLMs and allow your scripts to interact with them via the API. Below is demo of how to connect to LM studio and create generations in response to user queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(base_url=\"http://127.0.0.1:1234/v1\", api_key=\"lm-studio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm a language model so fine,\n",
      "Trained on data, with knowledge divine.\n",
      "I assist and answer, with speed and with care,\n",
      "To help users like you, with information to share.\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=\"model-identifier\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"Always answer in rhymes.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Introduce yourself.\"}\n",
    "  ],\n",
    "  # Temperature refers to the \"creativity\" of the response. \n",
    "  # This is accomplished via an algorithm known as soft max.\n",
    "  # It essentially increases the likelihood of less likely tokens being selected.\n",
    "  # A temperature of 0.0 will always pick the most likely token.\n",
    "  # A temperature of 1.0 will pick tokens based on their likelihood.\n",
    "  temperature=0.7,\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying towards the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT T1.Date_reported, T1.Cumulative_cases \n",
      "FROM Case AS T1 \n",
      "WHERE T1.Country_code IN ('US', 'USA') \n",
      "AND T1.Date_reported >= CURRENT_DATE - INTERVAL 7 DAY\n"
     ]
    }
   ],
   "source": [
    "schema =\"\"\"\n",
    "        Case (Date_reported, Country_code, Country, Continent, WHO_region, New_cases, Cumulative_cases, New_deaths, Cumulative_deaths)\n",
    "        Country (Country, latitude, longitude, name)\n",
    "        \"\"\"\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"model-identifier\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \n",
    "     \"content\": f\"\"\"\n",
    "                You are a SQL expert. You have the following schema: {schema}. \n",
    "                Allow a user to build SQL queries by taking a natural language query and returning a SQL query. \n",
    "                Respond with only a query.\n",
    "                \"\"\"},\n",
    "    {\"role\": \"user\", \"content\": \"Let me see new cases in the US for the last 7 days.\"}\n",
    "  ],\n",
    "  temperature=0.7,\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
